<!DOCTYPE html>
<html>
  <head>
    <title>Part 02 Developing and Operating </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Part 02<br/>Developing and Operating<br/><br/></br>

.debug[[shared/title.md](file:///home/xeon/urgent/slides/shared/title.md)]
---

class: title, in-person

Part 02<br/>Developing and Operating<br/><br/></br>



<!--
WiFi: **Something**<br/>
Password: **Something**

**Be kind to the WiFi!**<br/>
*Use the 5G network.*
*Don't use your hotspot.*<br/>
*Don't stream videos or download big files during the workshop*<br/>
*Thank you!*
-->

.debug[[shared/title.md](file:///home/xeon/urgent/slides/shared/title.md)]
---

name: toc-module-0

## Table of contents

- [Namespaces](#toc-namespaces)

- [Jobs](#toc-jobs)

- [Service and Selectors](#toc-service-and-selectors)

- [Ingress](#toc-ingress)

- [Healthchecks](#toc-healthchecks)

- [Rolling updates](#toc-rolling-updates)

- [Canary Deployments](#toc-canary-deployments)

- [Blue-green Deployments](#toc-blue-green-deployments)

- [Rolling back a Deployment](#toc-rolling-back-a-deployment)

- [Daemonsets & Statefulsets](#toc-daemonsets--statefulsets)

.debug[(auto-generated TOC)]



.debug[[shared/toc.md](file:///home/xeon/urgent/slides/shared/toc.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-namespaces
class: title

 Namespaces

.nav[
[Previous section](#toc-)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-jobs)
]

.debug[(automatically generated title slide)]

---

# Namespaces

- We can use `-n`/`--namespace` with almost every `kubectl` command

- Example:

  - `kubectl create --namespace=X` to create something in namespace X

- We can use `-A`/`--all-namespaces` with most commands that manipulate multiple objects

- Examples:

  - `kubectl delete` can delete resources across multiple namespaces

  - `kubectl label` can add/remove/update labels across multiple namespaces

.debug[[k8s/Namespaces.md](file:///home/xeon/urgent/slides/k8s/Namespaces.md)]
---
## What about `kube-public`?

.exercise[

- List the pods in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get pods
  ```

]

Nothing!

`kube-public` is created by kubeadm & [used for security bootstrapping](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters).

.debug[[k8s/Namespaces.md](file:///home/xeon/urgent/slides/k8s/Namespaces.md)]
---

## Exploring `kube-public`

- The only interesting object in `kube-public` is a ConfigMap named `cluster-info`

.exercise[

- List ConfigMap objects:
  ```bash
  kubectl -n kube-public get configmaps
  ```

- Inspect `cluster-info`:
  ```bash
  kubectl -n kube-public get configmap cluster-info -o yaml
  ```

]

Note the `selfLink` URI: `/api/v1/namespaces/kube-public/configmaps/cluster-info`

We can use that!

.debug[[k8s/Namespaces.md](file:///home/xeon/urgent/slides/k8s/Namespaces.md)]
---

## Accessing `cluster-info`

- Earlier, when trying to access the API server, we got a `Forbidden` message

- But `cluster-info` is readable by everyone (even without authentication)

.exercise[

- Retrieve `cluster-info`:
  ```bash
  curl -k https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info
  ```

]

- We were able to access `cluster-info` (without auth)

- It contains a `kubeconfig` file

---
.debug[[k8s/Namespaces.md](file:///home/xeon/urgent/slides/k8s/Namespaces.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-jobs
class: title

 Jobs

.nav[
[Previous section](#toc-namespaces)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-service-and-selectors)
]

.debug[(automatically generated title slide)]

---
# Jobs

- Deployments are great for stateless web apps

  (as well as workers that keep running forever)

- Pods are great for one-off execution that we don't care about

  (because they don't get automatically restarted if something goes wrong)

- Jobs are great for "long" background work

  ("long" being at least minutes our hours)

- CronJobs are great to schedule Jobs at regular intervals

  (just like the classic UNIX `cron` daemon with its `crontab` files)

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---

## Creating a Job

- A Job will create a Pod

- If the Pod fails, the Job will create another one

- The Job will keep trying until:

  - either a Pod succeeds,

  - or we hit the *backoff limit* of the Job (default=6)

.exercise[

- Create a Job that has a 50% chance of success:
  ```bash
    kubectl create job flipcoin --image=alpine -- sh -c 'exit $(($RANDOM%2))' 
  ```

]

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---

## Our Job in action

- Our Job will create a Pod named `flipcoin-xxxxx`

- If the Pod succeeds, the Job stops

- If the Pod fails, the Job creates another Pod

.exercise[

- Check the status of the Pod(s) created by the Job:
  ```bash
  kubectl get pods --selector=job-name=flipcoin
  ```

]

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---
## More advanced jobs

- We can specify a number of "completions" (default=1)

- This indicates how many times the Job must be executed

- We can specify the "parallelism" (default=1)

- This indicates how many Pods should be running in parallel

- These options cannot be specified with `kubectl create job`

  (we have to write our own YAML manifest to use them)

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---

## Scheduling periodic background work

- A Cron Job is a Job that will be executed at specific intervals

  (the name comes from the traditional cronjobs executed by the UNIX crond)

- It requires a *schedule*, represented as five space-separated fields:

  - minute [0,59]
  - hour [0,23]
  - day of the month [1,31]
  - month of the year [1,12]
  - day of the week ([0,6] with 0=Sunday)

- `*` means "all valid values"; `/N` means "every N"

- Example: `*/3 * * * *` means "every three minutes"

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---

## Creating a Cron Job

- Let's create a simple job to be executed every three minutes

- Careful: make sure that the job terminates!

  (The Cron Job will not hold if a previous job is still running)

.exercise[

- Create the Cron Job:
  ```bash
    kubectl create cronjob every3mins --schedule="*/3 * * * *" \
            --image=alpine -- sleep 10
  ```

- Check the resource that was created:
  ```bash
  kubectl get cronjobs
  ```

]

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---

## Cron Jobs in action

- At the specified schedule, the Cron Job will create a Job

- The Job will create a Pod

- The Job will make sure that the Pod completes

  (re-creating another one if it fails, for instance if its node fails)

.exercise[

- Check the Jobs that are created:
  ```bash
  kubectl get jobs
  ```

]

(It will take a few minutes before the first job is scheduled.)

.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---
## Setting a time limit

- It is possible to set a time limit (or deadline) for a job

- This is done with the field `spec.activeDeadlineSeconds`

  (by default, it is unlimited)

- When the job is older than this time limit, all its pods are terminated

- Note that there can also be a `spec.activeDeadlineSeconds` field in pods!

- They can be set independently, and have different effects:

  - the deadline of the job will stop the entire job

  - the deadline of the pod will only stop an individual pod


.debug[[k8s/batch-jobs.md](file:///home/xeon/urgent/slides/k8s/batch-jobs.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-service-and-selectors
class: title

 Service and Selectors

.nav[
[Previous section](#toc-jobs)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-ingress)
]

.debug[(automatically generated title slide)]

---
# Service and Selectors

--

  - ClusterIP: The default value. The service is only accessible from within the Kubernetes cluster

--

  - NodePort: This makes the service accessible on a static port on each Node in the cluster

--

  - Load Balancer: The service becomes accessible externally through a cloud provider's load balancer functionality. GCP, AWS, Azure, etc

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![serviceone](images/service01.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![serviceone](images/service02.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![serviceone](images/service03.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![serviceone](images/service04.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![serviceone](images/service05.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---


## Selector evaluation

- We can use selectors with many `kubectl` commands

- For instance, with `kubectl get`, `kubectl logs`, `kubectl delete` ... and more

.exercise[

- Get the list of pods matching selector `app=rng`:
  ```bash
  kubectl get pods -l app=rng
  kubectl get pods --selector app=rng
  ```

]

But ... why do these pods (in particular, the *new* ones) have this `app=rng` label?

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Where do labels come from?

- When we create a deployment with `kubectl create deployment rng`,
  <br/>this deployment gets the label `app=rng`

- The replica sets created by this deployment also get the label `app=rng`

- The pods created by these replica sets also get the label `app=rng`

- When we created the daemon set from the deployment, we re-used the same spec

- Therefore, the pods created by the daemon set get the same labels

.footnote[Note: when we use `kubectl run stuff`, the label is `run=stuff` instead.]

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Adding labels to pods

- We want to add the label `active=yes` to all pods that have `app=rng`

- We could edit each pod one by one with `kubectl edit` ...

- ... Or we could use `kubectl label` to label them all

- `kubectl label` can use selectors itself

.exercise[

- Add `active=yes` to all pods that have `app=rng`:
  ```bash
  kubectl label pods -l app=rng active=yes
  ```

]

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Labels and debugging

- When a pod is misbehaving, we can delete it: another one will be recreated

- But we can also change its labels

- It will be removed from the load balancer (it won't receive traffic anymore)

- Another pod will be recreated immediately

- But the problematic pod is still here, and we can inspect and debug it

- We can even re-add it to the rotation if necessary

  (Very useful to troubleshoot intermittent and elusive bugs)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Advanced label selectors

- Service selectors are limited to a `AND`

- But in many other places in the Kubernetes API, we can use complex selectors

  (e.g. Deployment, ReplicaSet, DaemonSet, NetworkPolicy ...)

- These allow extra operations; specifically:

  - checking for presence (or absence) of a label

  - checking if a label is (or is not) in a given set

- Relevant documentation:

  [Service spec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#servicespec-v1-core),
  [LabelSelector spec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#labelselector-v1-meta),
  [label selector doc](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Example of advanced selector

```yaml
  theSelector:
    matchLabels:
      app: portal
      component: api
    matchExpressions:
    - key: release
      operator: In
      values: [ production, preproduction ]
    - key: signed-off-by
      operator: Exists
```

This selector matches pods that meet *all* the indicated conditions.

`operator` can be `In`, `NotIn`, `Exists`, `DoesNotExist`.

A `nil` selector matches *nothing*, a `{}` selector matches *everything*.
<br/>
(Because that means "match all pods that meet at least zero condition".)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Services and Endpoints

- Each Service has a corresponding Endpoints resource

  (see `kubectl get endpoints` or `kubectl get ep`)

- That Endpoints resource is used by various controllers

  (e.g. `kube-proxy` when setting up `iptables` rules for ClusterIP services)

- These Endpoints are populated (and updated) with the Service selector

- We can update the Endpoints manually, but our changes will get overwritten

- ... Except if the Service selector is empty!

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![serviceone](images/service01.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-ingress
class: title

 Ingress

.nav[
[Previous section](#toc-service-and-selectors)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-healthchecks)
]

.debug[(automatically generated title slide)]

---

# Ingress

- *Services* give us a way to access a pod or a set of pods

- Services can be exposed to the outside world:

  - with type `NodePort` (on a port >30000)

  - with type `LoadBalancer` (allocating an external load balancer)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Ingress resources

- Kubernetes API resource (`kubectl get ingress`/`ingresses`/`ing`)

- Designed to expose HTTP services

- Basic features:

  - load balancing
  - SSL termination
  - name-based virtual hosting

- Can also route to different services depending on:

  - URI path (e.g. `/api`→`api-service`, `/static`→`assets-service`)
  - Client headers, including cookies (for A/B testing, canary deployment...)
  - and more!

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

## Principle of operation

- Step 1: deploy an *ingress controller*

  - ingress controller = load balancer + control loop

  - the control loop watches over ingress resources, and configures the LB accordingly

- Step 2: set up DNS

  - associate DNS entries with the load balancer address

- Step 3: create *ingress resources*

  - the ingress controller picks up these resources and configures the LB

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![ingress](images/rollout/ingress.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![ingress](images/rollout/nginx.png)

.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

![ingress](images/rollout/traefik.png)
.debug[[k8s/Services_Selectors.md](file:///home/xeon/urgent/slides/k8s/Services_Selectors.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-healthchecks
class: title

 Healthchecks

.nav[
[Previous section](#toc-ingress)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-rolling-updates)
]

.debug[(automatically generated title slide)]

---
# Healthchecks

- Kubernetes provides two kinds of healthchecks: liveness and readiness

- Healthchecks are *probes* that apply to *containers* (not to pods)

- Each container can have two (optional) probes:

  - liveness = is this container dead or alive?

  - readiness = is this container ready to serve traffic?

- Different probes are available (HTTP, TCP, program execution)

- Let's see the difference and how to use them!

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Liveness probe

- Indicates if the container is dead or alive

- A dead container cannot come back to life

- If the liveness probe fails, the container is killed

  (to make really sure that it's really dead; no zombies or undeads!)

- What happens next depends on the pod's `restartPolicy`:

  - `Never`: the container is not restarted

  - `OnFailure` or `Always`: the container is restarted

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## When to use a liveness probe

- To indicate failures that can't be recovered

  - deadlocks (causing all requests to time out)

  - internal corruption (causing all requests to error)

- Anything where our incident response would be "just restart/reboot it"

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Readiness probe

- Indicates if the container is ready to serve traffic

- If a container becomes "unready" it might be ready again soon

- If the readiness probe fails:

  - the container is *not* killed

  - if the pod is a member of a service, it is temporarily removed

  - it is re-added as soon as the readiness probe passes again

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## When to use a readiness probe

- To indicate failure due to an external cause

  - database is down or unreachable

  - mandatory auth or other backend service unavailable

- To indicate temporary failure or unavailability

  - application can only service *N* parallel connections

  - runtime is busy doing garbage collection or initial data load

- For processes that take a long time to start

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Timing and thresholds

- Probes are executed at intervals of `periodSeconds` (default: 10)

- The timeout for a probe is set with `timeoutSeconds` (default: 1)

.warning[If a probe takes longer than that, it is considered as a FAIL]

- A probe is considered successful after `successThreshold` successes (default: 1)

- A probe can have an `initialDelaySeconds` parameter (default: 0)

- Kubernetes will wait that amount of time before running the probe for the first time

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Different types of probes

- HTTP request

  - specify URL of the request (and optional headers)

  - any status code between 200 and 399 indicates success

- TCP connection

  - the probe succeeds if the TCP port is open

- arbitrary exec

  - a command is executed in the container

  - exit status of zero indicates success

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Benefits of using probes

- Rolling updates proceed when containers are *actually ready*

  (as opposed to merely started)

- Containers in a broken state get killed and restarted

  (instead of serving errors or timeouts)

- Unavailable backends get removed from load balancer rotation

  (thus improving response times across the board)

- If a probe is not defined, it's as if there was an "always successful" probe

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Example: HTTP probe


```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-demo
spec:
  containers:
  - name: demo
    image: demo/demo:v0.1
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 1
```

If the backend serves an error, or takes longer than 1s, 3 times in a row, it gets killed.

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Example: exec probe

Here is a pod template for a Redis server:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: redis-with-liveness
spec:
  containers:
  - name: redis
    image: redis
    livenessProbe:
      exec:
        command: ["redis-cli", "ping"]
```

If the Redis process becomes unresponsive, it will be killed.

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

## Example: Readiness Probe

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: readiness-pod
spec:
  containers:
  - name: app
    image: demo/app:v0.1
    ports:
    - containerPort: 3000
    readinessProbe:
      initialDelaySeconds: 2
      periodSeconds: 5
      httpGet:
        path: /ready
        port: 3000
```

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

.debug[[k8s/healthchecks.md](file:///home/xeon/urgent/slides/k8s/healthchecks.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-rolling-updates
class: title

 Rolling updates

.nav[
[Previous section](#toc-healthchecks)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-canary-deployments)
]

.debug[(automatically generated title slide)]

---
# Rolling updates

- Let's imagine you have your application deployed in Kubernetes. 

- You want to upgrade the app to version 2.0, and you wish to do so with a zero-downtime upgrade.

- The easier option is to add the newer version of the app alongside the existing ones and take out the old one.

- You will eventually migrate your production workload from version one to version two. 

- Replacing Pods one at the time is usually referred to as a rolling update.

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---
class: pic

![rollout](images/rollout/01.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/02.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/03.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/04.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/05.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/06.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/07.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/08.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/09.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/10.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/11.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/12.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/13.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/14.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/15.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/16.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/17.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

## This is what happens:

- Kubernetes will create a new Pod with the latest version

- Kubernetes will wait for the liveness probe to be healthy

- As soon as the readiness probe passes, the Pod is attached to the Service and is ready to receive traffic the Pod is ready

- Kubernetes will take down one of the Pods in the Deployment

- The same steps are repeated for every other Pod in the Deployment until the rollout is completed.

- Rolling updates are excellent when you wish to deliver features in production incrementally

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-canary-deployments
class: title

 Canary Deployments

.nav[
[Previous section](#toc-rolling-updates)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-blue-green-deployments)
]

.debug[(automatically generated title slide)]

---

# Canary Deployments

- Another option to deploy to production without disrupting live traffic is to use a Canary deployment.

- with a canary deployment you have two versions of your app (current and previous) deployed at the same time

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/18.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/19.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

## Using labels and selectors with Canary Deployments

- Create two Deployments: one for the existing app with Pods with a `version: 1.0.0` label and another with a `version: 2.0.0` label

- When you start, 100% of the traffic is routed to the existing application

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

```yaml

kind: Service
apiVersion: v1
metadata:
  name: canary-service
spec:
  selector:
    version: "1.0.0"
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
```

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

```yaml

kind: Service
apiVersion: v1
metadata:
  name: canary-service
spec:
  selector:
    component: frontend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
```

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/20.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/21.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

```yaml

kind: Service
apiVersion: v1
metadata:
  name: canary-service
spec:
  selector:
    version: "2.0.0"
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
```

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/22.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-blue-green-deployments
class: title

 Blue-green Deployments

.nav[
[Previous section](#toc-canary-deployments)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-rolling-back-a-deployment)
]

.debug[(automatically generated title slide)]

---

# Blue-green Deployments

- Rolling updates and Canary deployments are excellent strategies to introduce incremental updates

- However, there're times when you have to introduce breaking changes to your API or application

- In that case, you can't have two versions of your application live at the same time

- A more appropriate strategy to rollout breaking changes is to use a blue-green Deployment.

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/23.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/24.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/25.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/26.png)

- Don't forget to remove the Deployment for the previous app.

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-rolling-back-a-deployment
class: title

 Rolling back a Deployment

.nav[
[Previous section](#toc-blue-green-deployments)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-daemonsets--statefulsets)
]

.debug[(automatically generated title slide)]

---

# Rolling back a Deployment

- Things go wrong

- When you introduce a change that breaks production, you should have a plan to roll back that change

- Kubernetes and kubectl offer a simple mechanism to roll back changes to resources such as Deployments

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/26.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/27.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/28.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/29.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/30.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/31.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/32.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

![rollout](images/rollout/33.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---


class: pic

![rollout](images/rollout/34.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---


class: pic

![rollout](images/rollout/35.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---


class: pic

![rollout](images/rollout/36.png)

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

- keeping the previous ReplicaSets around is a convenient mechanism to roll back to a previously working version of your app

- By default Kubernetes stores the last 10 ReplicaSets and lets you roll back to any of them

- But you can change how many ReplicaSets should be retained by changing the spec.revisionHistoryLimit in your Deployment

```yaml
....
spec:
  replicas: 3
  revisionHistoryLimit: 100
selector:
  matchLabels:
    name: app
    .....
```
.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

.exercise[
Create a deployment for version 3.0.0

• the Deployment should have one replica

• the image should be ahmedgabercod/rollapp:3.0.0

• the Deployment should have readiness and liveness probes

• the Pods should have the following label: 
  version: 3.0.0

Can you change the service selector so that the traffic is routed only to version 2.0.0 and 3.0.0?
]

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

## Clean up

```bash
kubectl delete all --all

```
.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

.exercise[
Create a deployment for version 3.0.0

• the Deployment should have one replica

• the image should be ahmedgabercod/rollapp:3.0.0

• the Deployment should have readiness and liveness probes

• the Pods should have the following label: 
  version: 3.0.0

Can you change the selector for the service so that you can route traffic to version 1.0.0 , 2.0.0 and 3.0.0 with the following split?

- 1.0.0 50%
- 2.0.0 25%
- 3.0.0 25%
]

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

## Clean up

```bash
kubectl delete all --all

```

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

.exercise[
Create a deployment for version 3.0.0

• the Deployment should have one replica

• the image should be ahmedgabercod/rollapp:3.0.0

• the Deployment should have readiness and liveness probes

• the Pods should have the following label: 
  version: 3.0.0

You should upgrade your application to version blue-green deployment.
]

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

## Clean up

```bash
kubectl delete all --all

```

.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---


.debug[[k8s/rollout.md](file:///home/xeon/urgent/slides/k8s/rollout.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-daemonsets--statefulsets
class: title

 Daemonsets & Statefulsets

.nav[
[Previous section](#toc-rolling-back-a-deployment)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-)
]

.debug[(automatically generated title slide)]

---
# Daemonsets & Statefulsets

## Daemon sets in practice

- Daemon sets are great for cluster-wide, per-node processes:

  - `kube-proxy`

  - monitoring agents

  - etc.

- They can also be restricted to run [only on some nodes](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#running-pods-on-only-some-nodes)

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Creating a daemon set

  ```bash
  kubectl apply -f daemon-set.yaml
  ```

--

- How do we create the YAML file for our daemon set?

  [read the docs](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset)

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Statefulsets

- Stateful sets are a type of resource in the Kubernetes API

  (like pods, deployments, services...)

- They offer mechanisms to deploy scaled stateful applications

- At a first glance, they look like *deployments*:

  - a stateful set defines a pod spec and a number of replicas *R*

  - it will make sure that *R* copies of the pod are running

  - that number can be changed while the stateful set is running

  - updating the pod spec will cause a rolling update to happen

- But they also have some significant differences

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Stateful sets unique features

- Pods in a stateful set are numbered (from 0 to *R-1*) and ordered

- They are started and updated in order (from 0 to *R-1*)

- A pod is started (or updated) only when the previous one is ready

- They are stopped in reverse order (from *R-1* to 0)

- Each pod knows its identity (i.e. which number it is in the set)

- Each pod can discover the IP address of the others easily

- The pods can persist data on attached volumes

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Revisiting volumes

- [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) are used for many purposes:

  - sharing data between containers in a pod

  - exposing configuration information and secrets to containers

  - accessing storage systems

- Let's see examples of the latter usage

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Volumes types

- There are many [types of volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes) available:

  - public cloud storage (GCEPersistentDisk, AWSElasticBlockStore, AzureDisk...)

  - private cloud storage (Cinder, VsphereVolume...)

  - traditional storage systems (NFS, iSCSI, FC...)

  - distributed storage (Ceph, Glusterfs, Portworx...)

- Using a persistent volume requires:

  - creating the volume out-of-band (outside of the Kubernetes API)

  - referencing the volume in the pod description, with all its parameters

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Using a cloud volume

Here is a pod definition using an AWS EBS volume (that has to be created first):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-my-ebs-volume
spec:
  containers:
  - image: ...
    name: container-using-my-ebs-volume
    volumeMounts:
    - mountPath: /my-ebs
      name: my-ebs-volume
  volumes:
  - name: my-ebs-volume
    awsElasticBlockStore:
      volumeID: vol-049df61146c4d7901
      fsType: ext4
```

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Using an NFS volume

Here is another example using a volume on an NFS server:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-my-nfs-volume
spec:
  containers:
  - image: ...
    name: container-using-my-nfs-volume
    volumeMounts:
    - mountPath: /my-nfs
      name: my-nfs-volume
  volumes:
  - name: my-nfs-volume
    nfs:
      server: 192.168.0.55
      path: "/exports/assets"
```

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Individual volumes

- The Pods of a Stateful set can have individual volumes

  (i.e. in a Stateful set with 3 replicas, there will be 3 volumes)

- These volumes can be either:

  - allocated from a pool of pre-existing volumes (disks, partitions ...)

  - created dynamically using a storage system

- This introduces a bunch of new Kubernetes resource types:

  Persistent Volumes, Persistent Volume Claims, Storage Classes

  (and also `volumeClaimTemplates`, that appear within Stateful Set manifests!)

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Stateful set recap

- A Stateful sets manages a number of identical pods

  (like a Deployment)

- These pods are numbered, and started/upgraded/stopped in a specific order

- These pods are aware of their number

  (e.g., #0 can decide to be the primary, and #1 can be secondary)

- These pods can find the IP addresses of the other pods in the set

  (through a *headless service*)

- These pods can each have their own persistent storage

  (Deployments cannot do that)

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Persistent Volume Claims and Stateful sets

- A Stateful set can define one (or more) `volumeClaimTemplate`

- Each `volumeClaimTemplate` will create one Persistent Volume Claim per pod

- Each pod will therefore have its own individual volume

- These volumes are numbered (like the pods)

- Example:

  - a Stateful set is named `db`
  - it is scaled to replicas
  - it has a `volumeClaimTemplate` named `data`
  - then it will create pods `db-0`, `db-1`, `db-2`
  - these pods will have volumes named `data-db-0`, `data-db-1`, `data-db-2`

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## What's a Storage Class?

- A Storage Class is yet another Kubernetes API resource

  (visible with e.g. `kubectl get storageclass` or `kubectl get sc`)

- It indicates which *provisioner* to use

  (which controller will create the actual volume)

- And arbitrary parameters for that provisioner

  (replication levels, type of disk ... anything relevant!)

- Storage Classes are required if we want to use [dynamic provisioning](https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/)

  (but we can also create volumes manually, and ignore Storage Classes)

.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---

## Dynamic provisioning usage

After setting up the system, all we need to do is:

*Create a Stateful Set that makes use of a `volumeClaimTemplate`.*

This will trigger the following actions.

1. The Stateful Set creates PVCs according to the `volumeClaimTemplate`.

2. The Stateful Set creates Pods using these PVCs.

3. The PVCs are automatically annotated with our Storage Class.

4. The dynamic provisioner provisions volumes and creates the corresponding PVs.

5. The PersistentVolumeClaimBinder associates the PVs and the PVCs together.

6. PVCs are now bound, the Pods can start.



.debug[[k8s/statefulsets.md](file:///home/xeon/urgent/slides/k8s/statefulsets.md)]
---
class: title, in-person

Thank you! <br/> Questions?

.debug[[shared/thankyou.md](file:///home/xeon/urgent/slides/shared/thankyou.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        slideNumberFormat: '%current%/%total%',
        excludedClasses: ["self-paced","snap","btp-auto","benchmarking","elk-manual","prom-manual","extra-details"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
